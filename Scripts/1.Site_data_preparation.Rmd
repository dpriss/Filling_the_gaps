---
title: "Data Preparation for Connecting Algorithm"
author: "Deborah Priß"
date: "2022-10-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

sessionInfo()
R version 4.1.0 (2021-05-18)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19045)

Matrix products: default

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252    LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                    LC_TIME=German_Germany.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] plotly_4.10.0   tictoc_1.0.1    rgeos_0.5-9     sp_1.5-0        nngeo_0.4.6     forcats_0.5.2   stringr_1.4.1   dplyr_1.0.9     purrr_0.3.4     readr_2.1.3    
[11] tidyr_1.2.0     tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.2 plyr_1.8.7      sf_1.0-7       

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.9          lubridate_1.8.0     lattice_0.20-44     class_7.3-19        assertthat_0.2.1    digest_0.6.29       utf8_1.2.2          R6_2.5.1           
 [9] cellranger_1.1.0    backports_1.4.1     reprex_2.0.2        evaluate_0.17       e1071_1.7-11        httr_1.4.4          pillar_1.8.1        rlang_1.0.6        
[17] lazyeval_0.2.2      googlesheets4_1.0.1 readxl_1.4.1        rstudioapi_0.14     data.table_1.14.2   googledrive_2.0.0   htmlwidgets_1.5.4   munsell_0.5.0      
[25] proxy_0.4-27        broom_1.0.1         compiler_4.1.0      modelr_0.1.9        xfun_0.31           pkgconfig_2.0.3     htmltools_0.5.2     tidyselect_1.1.2   
[33] viridisLite_0.4.1   fansi_1.0.3         crayon_1.5.2        tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0         grid_4.1.0          jsonlite_1.8.0     
[41] gtable_0.3.1        lifecycle_1.0.3     DBI_1.1.3           magrittr_2.0.3      units_0.8-0         scales_1.2.1        KernSmooth_2.23-20  cli_3.4.1          
[49] stringi_1.7.8       farver_2.1.1        fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2      generics_0.1.3      vctrs_0.4.1         tools_4.1.0        
[57] glue_1.6.2          hms_1.1.2           fastmap_1.1.0       colorspace_2.0-3    gargle_1.2.1        classInt_0.4-7      rvest_1.0.3         knitr_1.40         
[65] haven_2.5.1


## Introduction

This script provides the code and instructions to prepare data sets to use with the algorithm to connect Mesopotamian hollow ways with each other and with their adjacent sites.

The Mesopotamian hollow data set can be downloaded here <https://services1.arcgis.com/qN3V93cYGMKQCOxL/arcgis/rest/services/Hollow_Ways_PUBLIC/FeatureServer>. The site data from ANE is available here <http://ancientworldonline.blogspot.com/2011/07/ane-placemarks-for-google-earth.html> and the site polygons generated by Tuna Kalaycı here <https://zenodo.org/record/7048407>. The data from the Fragile Crescent Project (FCP) are not publicly available but the subset used here can be downloaded from this repository.

The aim of this data preparation is to merge the site data sets in a reasonable, i.e. to have a resulting data set that includes as many sites and attributes as possible without generating overlapping sites and/or artifacts.

## Environment set up

First, the necessary packages need to be installed and loaded.

```{r Install and load packages }

# install.packages("Rtools")
# library(Rtools)
# install.packages("sf")
# install.packages("ggplot2")
# install.packages("tidyverse")
# install.packages("nngeo")
# install.packages("dplyr")
# install.packages("rgeos")
# install.packages("sp")
# install.packages("plotly")
# install.packages("tictoc")

library(sf)
library(plyr)
library(tidyverse)
library(nngeo)
library(dplyr)
library(rgeos)
library(sp)
library(plotly)

#switch spherical geometry off because it causes problems with some functions
sf::sf_use_s2(FALSE)
```

And the data sets in the form of shapefiles need to be imported. Note that files in .csv or .xlsx/.xls can be imported as well and then converted into spatial objects as long as they have either a geometry column or columns containing the XY-coordinates. To have all data sets in the same coordinate systems, they are imported using st_transform (crs 4326 = WGS84). The data also needs to be converted into sf objects with st_sf.

```{r Import and convert data}

#import the sites predicted by the Matlab algorithm (see Priß....)
conf_sites <- st_transform(st_read("conf_sites_32637.shp"), crs = 32637)

#import ANE sites, already clipped to extent of hollow ways in ArcPro and with additional information of dating and size as far as available
Ane <- st_transform(st_read("Ane_clip_tight_32637.shp"), crs = 32637)

#importTuna's polygons of sites, remove one empty geometry and cast them to polygons
Tuna <- st_read("Tuna_Clip.shp")

#import FCP data 
Khabur <-  st_transform(st_read("BA_IA_Sites_Names_Sizes.shp"), 32637)


#plot the data with ggplot and plotly. Note that plotly does not work in the .Rmd environment because it requires the Viewer. To plot with plotly, copy-paste the code to your console.
ggplot() + 
  geom_sf(data = Khabur, color = "red") +
  coord_sf() +
  geom_sf(data = conf_sites, color = "blue") +
  geom_sf(data = Ane, color = "green") +
  geom_sf(data = Tuna, color = "purple", fill = "purple")

# plot_ly() %>%
#   add_sf(data = Khabur) %>%
#   add_sf(data = conf_sites_sf) %>%
#   add_sf(data = Ane) %>%
#   add_sf(data = Tuna)
```

## 

## Data Preparation

The next steps are to manipulate the loaded data in a way that the different data sets and their attribute columns are comparable so that they can be easily merged. The following chunks show this process for every file individually with explanations as comments.

Some sites in the FCP data set are listed several times if they existed in more than one period, i.e. they are represented by as many points as the periods they existed in and have one point for every separately recorded part of the site. This led to an accumulation of points for certain sites which in turn caused a fuzzy and inaccurate picture. Therefore, those sites are merged by using the information in the original data set.

Every entry in the data set has an ID and a Major_ID. This Major_ID identifies the site and the survey and is extended when the site is divided into more than one part. For example, Tell Leilan has the Major_ID LLN_1\_0_0 whereby LLN is the abbreviation for the Leilan Regional Survey and 1 is the main ID for the site. Besides the main site, there is another entry for "Tell Leilan, Tell" with the Major_ID LLN_1\_1_0 and one for "Tell Leilan, Lower Town" with the Major_ID LLN_1\_2_0. Those and other entries were merged based on the first characters of the Major_ID, in the case of Tell Leilan LLN_1, and all relevant information from the individual entries were kept. This procedure reduced the data set for the whole region from 1257 to 489 sites. The data set was then filtered by the broader periods (EBA1, EBA2, MBA, LBA, IA1, IA2) and stored in separate files, one for each period.

I had already added some inforamation to the original FCP data set like coordinates and additional site sizes provided by Tuna Kalaycı. To work with the original FCP sites without losing the relevant supplements, we now need to remove the unnecessary sites, i.e. merge those that are listed several times for different periods.

```{r Edit: all files}

#add column "confidence level" to all data sets to facilitate tracing them back to the original data set in the end.
Khabur$confidence_level <- 1
Tuna$confidence_level <- 2
Ane$confidence_level <- 3
conf_sites_sf$confidence_level <- 4

##ANE: replace NA values with 0. The geometry column interferes with this process, so it is dropped first and then added again after the values are replaced.
Ane_no_geometry <- st_drop_geometry(Ane) 
Ane_no_geometry[is.na(Ane_no_geometry)] <- 0
Ane <- st_set_geometry(Ane_no_geometry, Ane$geometry)
##change column name LBA to make it consistent with FCP data set 
names(Ane)[names(Ane) == 'LBA'] <- 'LBA_broad'
```

```{r Edit: Tuna}

#create buffer around Tuna's polygons as some of the sites lie slightly outside. Transformation to Syrian grid (22770) necessary to avoid errors.
Tuna_trans <- st_transform(Tuna, 22770) 
Tuna_buffer <- st_buffer(Tuna_trans, dist = 200)
##transform back to WGS84
Tuna_buf_trans <- st_transform(Tuna_buffer, crs = 32637)

# plot_ly() %>%
#   add_sf(data = Khabur) %>%
#   add_sf(data = conf_sites_sf) %>%
#   add_sf(data = Ane) %>%
#   add_sf(data = Tuna_buf_trans)

#adding column "ID" to Tuna's data
Tuna_buf_trans$ID <- seq(1, nrow(Tuna), by = 1)

```

```{r Edit: FCP}

#clean the data set 
##remove irrelevant columns 
sites_sf <- Khabur_complete_sf[ , -which(names(Khabur_complete_sf) %in% c("L1","L2", "L3", "Sort_ID", "ORIG_FID", "Major_ID_1"))]
##add necessary columns
sites_sf$FID <- seq(1, nrow(sites_sf), by = 1)

#Step 1: Merge Sites

#replace -9999 by 0.0001 (sites without size but with dating information) - replacing with 0 would cause sites to disappear in certain periods, replacing with a character doesn't work (error) and replacing with NA causes the summarise function to return wrong results.
##the geometry column causes the replacing to throw an error, so it needs to be removed before and put back afterwards
sites_no_geometry <- st_drop_geometry (sites_sf)
sites_no_geometry [sites_no_geometry == -9999] <- 0.0001  
sites_sf <- st_set_geometry(sites_no_geometry, Khabur_complete_sf$geometry) 

#for every entry in the data set, check if there are others that have the same characters in "Major_ID" up to the second underscore 
##extract the general site ID for every entry and add it in a new column "Site_ID"
for (i in 1:length(sites_sf)) { 
  sites_sf$Site_ID <- str_extract(sites_sf$Major_ID, "[A-Z]*_[0-9]*") 
}

#merge the sites
Khabur_ID_merge <- NULL
Khabur_ID_merge <- sites_sf %>%
  #use the extracted major site ID as the condition on which to merge the sites
  group_by(Site_ID) %>%
  #summarise data set: minimum of FID, Dataset, Major_ID, Tuna_area, minimum other than NA of Site_Name, 
  #sums of periods (parts of sites active during period), average of coordinates (centroids)
  dplyr::summarise(across(c(FID, Dataset, Major_ID), list(min)), Site_Name = min(Site_Name, na.rm = T), 
            EJZ1 = sum(EJZ1), EJZ1_2 = sum(EJZ1_2), EJZ2 = sum(EJZ2), NINV = sum(NINV), EJZ3a = sum(EJZ3a),
            EJZ3 = sum(EJZ3), EJZ3b = sum(EJZ3b), AKK = sum(AKK), EJZ4 = sum(EJZ4), EJZ4C = sum(EJZ4C),
            LTM = sum(LTM), EBA_FCP = sum(EBA_FCP), KHA = sum(KHA), MBA_LBA_T = sum(MBA_LBA_T), 
            NUZI = sum(NUZI), LBA = sum(LBA), SM = sum(SM), MASS = sum(MASS), EIA_FCP = sum(EIA_FCP),
            LASS = sum(LASS), MIA_FCP = sum(MIA_FCP), EIA_MIA_FC = sum(EIA_MIA_FC), IA = sum(IA),
            IA_FCP = sum(IA_FCP), LIA = sum(LIA), PASS = sum(PASS), POINT_X = mean(POINT_X), POINT_Y = mean(POINT_Y), 
            confidence_level = max(confidence_level))

#convert tibble to data frame and then to sf object 
Khabur_ID_merge_df <- as.data.frame(Khabur_ID_merge)
Khabur_ID_merge_sf <- st_as_sf(Khabur_ID_merge_df, coords = c("POINT_X", "POINT_Y"), remove = FALSE, crs = 32637)

#compare the merged sites with the original data 
ggplot() +
  geom_sf(data = sites_sf, color = "red") +
  geom_sf(data = Khabur_ID_merge_sf, color = "blue") 

# plot_ly() %>%
#   add_sf(data = sites_sf) %>%
#   add_sf(data = Khabur_ID_merge_sf) 



#Step 2: Broader periods

#create columns to merge the sub-periods into broader periods 
Khabur_ID_merge_periods <- Khabur_ID_merge_sf %>%
  rowwise() %>%
  mutate(EBA1 = case_when(EBA_FCP != 0 | NINV != 0 | EJZ1 != 0 | EJZ1_2 != 0 ~ pmax(EBA_FCP, NINV, EJZ1, EJZ1_2), 
                          TRUE ~ 0)) %>%
  mutate(EBA2 = case_when(EBA_FCP != 0 | EJZ2 != 0 | EJZ3 != 0 | EJZ3a != 0 | EJZ3b != 0 | EJZ4 != 0 | EJZ4C != 0 | 
                          LTM != 0 | AKK != 0 ~ pmax(EBA_FCP, EJZ2, EJZ3, EJZ3a, EJZ3b, EJZ4, EJZ4C, LTM, AKK), 
                          TRUE ~ 0)) %>%
  mutate(MBA = case_when(KHA != 0 | SM != 0 | MBA_LBA_T != 0 ~ pmax(MBA_LBA_T, SM, KHA), 
                         TRUE ~ 0)) %>%
  mutate(LBA_broad = case_when(LBA != 0 | SM != 0 | MBA_LBA_T != 0 | NUZI != 0 | MASS != 0 ~ pmax(LBA, SM, MBA_LBA_T, 
                                                                                                  NUZI, MASS), 
                               TRUE ~ 0)) %>%
  mutate(IA1 = case_when(MASS != 0 | EIA_FCP != 0 | IA != 0 | EIA_MIA_FC != 0 ~ pmax(MASS, EIA_FCP, IA, EIA_MIA_FC), 
                         TRUE ~ 0)) %>%
  mutate(IA2 = case_when(EIA_FCP != 0 | IA != 0 | EIA_MIA_FC != 0 | LASS != 0 | MIA_FCP != 0 | LIA != 0 | PASS != 0 ~
                       pmax(EIA_FCP, IA, EIA_MIA_FC, LASS, MIA_FCP, LIA, PASS), 
                       TRUE ~ 0))

#generate period-specific data sets and add column for average size and size category
Khabur_ID_merge_EBA1 <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  #filter all sites that have size information for EBA1 (including those without size, i.e. 0.0001)
  filter(EBA1 != 0) %>%
  #add new column "Size": if there is size information, take this value; all sites, that have 0.0001 or a multiple of it (i.e. no size but dating information), will get NA.
  mutate(Size = case_when(EBA1 == 0.0001 | EBA1 == 0.0002 | EBA1 == 0.0003 | EBA1 == 0.0004 | EBA1 == 0.0005 | EBA1 == 0.0006 
                          | EBA1 == 0.0007 ~ NA_real_, 
                          TRUE ~ EBA1)) %>%
  #add size categories
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))

Khabur_ID_merge_EBA2 <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  filter(EBA2 != 0) %>%
  mutate(Size = case_when(EBA2 == 0.0001 | EBA2 == 0.0002 | EBA2 == 0.0003 | EBA2 == 0.0004 | EBA2 == 0.0005 | EBA2 == 0.0006 
                          | EBA2 == 0.0007 ~ NA_real_, 
                          TRUE ~ EBA2)) %>%
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))

Khabur_ID_merge_MBA <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  filter(MBA != 0) %>%
  mutate(Size = case_when(MBA == 0.0001 | MBA == 0.0002 | MBA == 0.0003 | MBA == 0.0004 | MBA == 0.0005 | MBA == 0.0006 
                          | MBA == 0.0007 ~ NA_real_, 
                          TRUE ~ MBA)) %>%
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))
  
Khabur_ID_merge_LBA <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  filter(LBA_broad != 0) %>%
  mutate(Size = case_when(LBA_broad == 0.0001 | LBA_broad == 0.0002 | LBA_broad == 0.0003 | LBA_broad == 0.0004 | 
                          LBA_broad == 0.0005 | LBA_broad == 0.0006 | LBA_broad == 0.0007 ~ NA_real_, 
                          TRUE ~ LBA_broad)) %>%
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))

Khabur_ID_merge_IA1 <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  filter(IA1 != 0) %>%
  mutate(Size = case_when(IA1 == 0.0001 | IA1 == 0.0002 | IA1 == 0.0003 | IA1 == 0.0004 | IA1 == 0.0005 | IA1 == 0.0006
                          | IA1 == 0.0007 ~ NA_real_,
                          TRUE ~ IA1)) %>%
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))

Khabur_ID_merge_IA2 <- Khabur_ID_merge_periods %>%
  rowwise() %>%
  filter(IA2 != 0) %>%
  mutate(Size = case_when(IA2 == 0.0001 | IA2 == 0.0002 | IA2 == 0.0003 | IA2 == 0.0004 | IA2 == 0.0005 | IA2 == 0.0006
                          | IA2 == 0.0007 ~ NA_real_,
                          TRUE ~ IA2)) %>%
  mutate(SizeCat = case_when(Size > 0 && Size <= 1 ~ "Under 1ha",
                             Size > 1 && Size <= 5 ~ "1ha - 5ha",
                             Size > 5 && Size <= 10 ~ "5ha - 10ha",
                             Size > 10 && Size <= 25 ~ "10 ha - 25ha",
                             Size > 25 && Size <= 50 ~ "25ha - 50ha",
                             Size > 50 ~ "Over 50ha",
                             TRUE ~ "No size"))

##replace the "#N/A" entries for site names in the original data set with NA to make it easier to work with them. 
Khabur_ID_merge_periods_no_geometry <- st_drop_geometry(Khabur_ID_merge_periods) 
Khabur_ID_merge_periods_no_geometry$Site_Name[Khabur_ID_merge_periods_no_geometry$Site_Name == "#N/A"] <- NA
Khabur_ID_merge_periods <- st_set_geometry(Khabur_ID_merge_periods_no_geometry, Khabur_ID_merge_periods$geometry)

# #Step 3: Write Files
# st_write(sites_sf, dsn = "Khabur_sites.shp")
# st_write(Khabur_ID_merge_sf, dsn = "Khabur_ID_merge.shp")
# st_write(Khabur_ID_merge_periods, dsn = "Khabur_ID_merge_periods.shp")
# 
# st_write(sites_sf, dsn = "Khabur_sites.csv")
# st_write(Khabur_ID_merge_sf, dsn = "Khabur_ID_merge.csv")
# st_write(Khabur_ID_merge_periods, dsn = "Khabur_ID_merge_periods.csv")
# 
# st_write(Khabur_ID_merge_EBA1, dsn = "Khabur_ID_merge_EBA1.shp")
# st_write(Khabur_ID_merge_EBA2, dsn = "Khabur_ID_merge_EBA2.shp")
# st_write(Khabur_ID_merge_MBA , dsn = "Khabur_ID_merge_MBA.shp")
# st_write(Khabur_ID_merge_LBA, dsn = "Khabur_ID_merge_LBA.shp")
# st_write(Khabur_ID_merge_IA1 , dsn = "Khabur_ID_merge_IA1.shp")
# st_write(Khabur_ID_merge_IA2 , dsn = "Khabur_ID_merge_IA2.shp")
# 
# st_write(Khabur_ID_merge_EBA1, dsn = "Khabur_ID_merge_EBA1.csv")
# st_write(Khabur_ID_merge_EBA2, dsn = "Khabur_ID_merge_EBA2.csv")
# st_write(Khabur_ID_merge_MBA , dsn = "Khabur_ID_merge_MBA.csv")
# st_write(Khabur_ID_merge_LBA, dsn = "Khabur_ID_merge_LBA.csv")
# st_write(Khabur_ID_merge_IA1 , dsn = "Khabur_ID_merge_IA1.csv")
# st_write(Khabur_ID_merge_IA2 , dsn = "Khabur_ID_merge_IA2.csv")

#Step 4: Further Preparation
##convert the data set we will use now into an sf object
Khabur_sf <- st_as_sf(Khabur_ID_merge_periods)

#add Tuna's size to FCP site if the site is in the buffer around Tuna's ploygon
Khabur_sf$Tuna_size <- apply(st_intersects(Tuna_buf_trans, Khabur_sf, sparse = FALSE), 2, 
                                   function(col) { 
                                     Tuna_buf_trans[which(col), ]$Shape_Area
                                   })

##replace numeric(0) values, i.e. when no size was available, with 0
Khabur_sf$Tuna_size[Khabur_sf$Tuna_size == "numeric(0)"] <- 0

#define which FCP site lies in which Tuna polygon and add the respective IDs to the FCP data set
Khabur_sf$Tuna_polygon <- apply(st_intersects(Tuna_buf_trans, Khabur_sf, sparse = FALSE), 2, 
                       function(col) { 
                         Tuna_buf_trans[which(col), ]$ID
                       })
###replace numeric(0) values
Khabur_sf$Tuna_polygon[Khabur_sf$Tuna_polygon == "numeric(0)"] <- 0
###unlisting the column as values have been stored in a list rather then numerics
Khabur_sf$Tuna_polygon <- unlist(Khabur_sf$Tuna_polygon)
```

```{r Edit: predicted sites}

#subset predicted sites to those that are completely new
conf_sites_new <- conf_sites_sf %>%
  filter(Confirmed == "Yes" &
         FCP == "No" &
         ANE == "No" &
         Tuna == "No")
```

```{r Edit: ANE}

#add Tuna's size to ANE site if the site is in the buffer around Tuna's ploygon
Ane$Tuna_size <- apply(st_intersects(Tuna_buf_trans, Ane, sparse = FALSE), 2, 
                                   function(col) { 
                                     Tuna_buf_trans[which(col), ]$Shape_Area
                                   })

#replace numeric(0) values, i.e. when no size was available, with 0
Ane$Tuna_size[Ane$Tuna_size == "numeric(0)"] <- 0

#unlist Tuna_size column because the next operations do not work on lists
Ane <- unnest(Ane, "Tuna_size")

#add either Tuna_size or size from online search (already been added as "Size")
##change confidence level from 3 to 2 if Tuna_size is available
##replace 0.0001 (dating known, size unknown) and 0 (neither size nor dating known) with NA
Ane <- Ane %>%
  rowwise() %>%
  mutate(Size = case_when(Tuna_size != 0 ~ Tuna_size,
                          EBA1 != 0 ~ EBA1,
                          EBA2 != 0 ~ EBA2,
                          MBA != 0 ~ MBA,
                          LBA_broad != 0 ~ LBA_broad,
                          IA1 != 0 ~ IA1,
                          IA2 != 0 ~ IA2,
                          TRUE ~ 0)) %>%
  mutate(confidence_level  = case_when(Size == Tuna_size ~ 2,
                                       TRUE ~ 3)) %>%
  mutate(Size = replace(Size, Size == 0.0001 | Size == 0, NA))

#define which ANE site lies in which Tuna polygon and add the respective IDs to the ANE data set
Ane$Tuna_polygon <- apply(st_intersects(Tuna_buf_trans, Ane, sparse = FALSE), 2, 
                       function(col) { 
                         Tuna_buf_trans[which(col), ]$ID
                       })

###replace numeric(0) values
Ane$Tuna_polygon[Ane$Tuna_polygon == "numeric(0)"] <- 0
###unlisting the column as values have been stored in a list rather then numerics
Ane$Tuna_polygon <- unlist(Ane$Tuna_polygon)
```

## Merging the Data

Now, the data is appropriately prepared, i.e. the columns are comparable as far as possible and the attributes (size, dating) is stored in a way to have as much additional information as possible. Missing data are a huge problem for network analysis, especially social network models like ERGMs or SAOMs, therefore it is advisable to try to overcome this issue as early and as reasonable as possible.

The next steps will illustrate how the data sets were merged.

```{r Merge: ANE and FCP}

#for loop to add site name from ANE to FCP if there is no name in FCP
##for every entry in Khabur_sf
##Note that this will run for some minutes
for (i in 1:nrow(Khabur_sf)) {
  ##and for every entry in Ane
  for (j in 1:nrow(Ane))
    ##if they are in the same polygon, i.e. if the numbers in their respective Tuna_polygon columns match and if the Site_Nm column in Khabur_sf is NA.
  if (Khabur_sf[i,]$Tuna_polygon == Ane[j,]$Tuna_polygon && is.na(Khabur_sf[i,]$Site_Name)) {
    ##if the numbers in Tuna_polygon are not 0, i.e. the sites do not lie in a polygon
    if (Khabur_sf[i,]$Tuna_polygon != 0 && Ane[j,]$Tuna_polygon != 0) {
      ##add the site name stored in ANE to the FCP data
      Khabur_sf[i,]$Site_Name <- Ane[j,]$Name
    }
  }
}

#add ANE sites to FCP if they don't correlate with FCP sites, i.e. if the polygons number are neither same nor 0
Khabur_Ane <- rbind.fill(Khabur_sf, Ane[Ane$Tuna_polygon == 0 | !Ane$Tuna_polygon %in% (Khabur_sf$Tuna_polygon), ])

# #replace "NA" or "" in the data set with NA
# Khabur_Ane$Period[Khabur_Ane$Period == "NA" | Khabur_Ane$Period == ""] <- NA

#add Period column 
Khabur_Ane$Period = NA
for (i in 1:nrow(Khabur_Ane)) {
  ##if the value in the respective period column (EBA1, EBA2...) is not zero, i.e. the site existed in that period
  if (Khabur_Ane$EBA1[i] != 0) {
    ##add the name of the period to the period column if the value is not NA
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("EBA1")
    }
    else {
      ##or append to the list if there is already a value there
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "EBA1")
    }
  }
  if (Khabur_Ane$EBA2[i] != 0) {
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("EBA2")
    }
    else {
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "EBA2")
    }
  }
  if (Khabur_Ane$MBA[i] != 0) {
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("MBA")
    }
    else {
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "MBA")
    }
  }
  if (Khabur_Ane$LBA_broad[i] != 0) {
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("LBA")
    }
    else {
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "LBA")
    }
  }
  if (Khabur_Ane$IA1[i] != 0) {
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("IA1")
    }
    else {
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "IA1")
    }
  }
  if (Khabur_Ane$IA2[i] != 0) {
    if (is.na(Khabur_Ane$Period[[i]][[1]])) {
      Khabur_Ane$Period[[i]] <- list("IA2")
    }
    else {
      Khabur_Ane$Period[[i]][[1]] <- append(Khabur_Ane$Period[[i]][[1]], "IA2")
    }
  }
}

#unlist the "Period" column because otherwise it cannot be saved as .shp or .csv
Khabur_Ane["Period"] <- apply(Khabur_Ane["Period"], 2, function(y) sapply(y, function(x) paste(unlist(x), collapse=",")))
Khabur_Ane["Tuna_size"] <- apply(Khabur_Ane["Tuna_size"], 2, function(y) sapply(y, function(x) paste(unlist(x), collapse=",")))


#replace "NA" with NA (caused by unlisting)
Khabur_Ane["Period"][Khabur_Ane["Period"] == "NA"] <- NA
Khabur_Ane["Tuna_size"][Khabur_Ane["Period"] == "Tuna_size"] <- NA

#removing geometry because it interferes with the for loop to change the Size column.
Khabur_geom <- Khabur_Ane$geometry
Khabur_ng <- subset(Khabur_Ane, select = -c(geometry) )

#renew "Size" column: add size information from FCP and ANE to "Size" column
for (i in 1:nrow(Khabur_ng)) {
  if (is.na(Khabur_ng[i,]$Size)) {
    if (Khabur_ng[i,]$EBA1 != 0 | Khabur_ng[i,]$EBA2 != 0 | Khabur_ng[i,]$MBA != 0 | Khabur_ng[i,]$LBA_broad != 0 | Khabur_ng[i,]$IA1 != 0 | Khabur_ng[i,]$IA2 != 0)
      Khabur_ng[i,]$Size = max(Khabur_ng[i,]$EBA1,Khabur_ng[i,]$EBA2, Khabur_ng[i,]$MBA, Khabur_ng[i,]$LBA_broad, Khabur_ng[i,]$IA1, Khabur_ng[i,]$IA2)
  }
}

#adding geometry back
Khabur_Ane <- st_set_geometry(Khabur_ng, Khabur_geom)

#change multiples of 0.0001 (added above for indicating dating without size) to 0 and add Tuna_size in case there is no size information in the FCP data set
##the column Tuna_size consisted of characters, so it needs to be change to numerics first
Khabur_Ane$Tuna_size <- as.numeric(Khabur_Ane$Tuna_size)
Khabur_Ane <- Khabur_Ane %>%
  rowwise() %>%
  mutate(Size = case_when(Size == 0.0001 | Size == 0.0002 | Size == 0.0003 | Size == 0.0004 | Size == 0.0005 | Size == 0.0006
                          | Size == 0.0007 ~ 0,
                          TRUE ~ Size)) %>%
  mutate(Size = case_when(Size == 0 ~ Tuna_size,
                          TRUE ~ Size))

#replace 0s with NA
Khabur_Ane$Size[Khabur_Ane$Size == 0] <- NA
```

```{r Merge: FCP/ANE and predicted sites}

#add predicted sites to the data set. As there is not much information about them despite their location and the CORONA satellite image on which they can be found, the rows are simply added to the end of the file.
Khabur_Ane_pred <- rbind.fill(Khabur_Ane, conf_sites_new)

#removing geometry because it interferes with the for loop to change the Size column.
Khabur_geom <- Khabur_Ane_pred$geometry
Khabur_ng <- subset(Khabur_Ane_pred, select = -c(geometry) )

#remove unnecessary columns
Khabur_ng <- subset(Khabur_ng, select = -c(47:50))

#adding geometry back
Khabur_Ane_pred <- st_set_geometry(Khabur_ng, Khabur_geom)
```

```{r Merge: FCP/ANE/predicted and Tuna}

#add Tuna's sites to the data set if they're not already in there
##extract centroids from sites
Tuna_centroids <- st_centroid(Tuna_buf_trans)
##clean columns (removing unnecessary ones and renaming others to match the FCP data set)
Tuna_centroids <- subset(Tuna_centroids, select = -c(3:15))
names(Tuna_centroids)[names(Tuna_centroids) == 'CENTROID_X'] <- 'POINT_X'
names(Tuna_centroids)[names(Tuna_centroids) == 'CENTROID_Y'] <- 'POINT_Y'
names(Tuna_centroids)[names(Tuna_centroids) == 'Shape_Area'] <- 'Size'
names(Tuna_centroids)[names(Tuna_centroids) == 'ID'] <- 'Tuna_polygon'

#add Tuna's site to the extended FCP data set if the number of the polygon is not already listed in the extended FCP file
Khabur_Ane_pred_Tuna <- rbind.fill(Khabur_Ane_pred, Tuna_centroids[!Tuna_centroids$Tuna_polygon %in% (Khabur_Ane_pred$Tuna_polygon), ])

Khabur_Ane_pred_Tuna <- st_sf(Khabur_Ane_pred_Tuna)

#clean data set to make everything consistent
##replace 0s with NA
Khabur_Ane_pred_Tuna$Tuna_polygon[Khabur_Ane_pred_Tuna$Tuna_polygon == 0] <- NA
###making sure the site names are up-to-date
Khabur_Ane_pred_Tuna <- Khabur_Ane_pred_Tuna %>%
  mutate(Site_Name = case_when(is.na(Site_Name) && !is.na(Name) ~ Name,
                             TRUE ~ Site_Name))

#converting list column "Period" to characters to avoid issues with subsetting and writing to .shp
Khabur_Ane_pred_Tuna$Period <- as.character(Khabur_Ane_pred_Tuna$Period)

#changing coordinate system to avoid error when subsetting
Khabur_Ane_pred_Tuna <- st_transform(Khabur_Ane_pred_Tuna, crs = 32637)

#write the data set to a shapefile
#st_write(Khabur_Ane_pred_Tuna_sf, dsn = "Khabur_Ane_pred_Tuna.shp")
```

## Period Subsets

As an optional last step, the data set can be split into the respective periods so that they can be put into the connecting algorithm individually. For this project, it is necessary to have this separation to explore how the network changes over time. However, there are still sites without dating information which are excluded or included respectively in the period-specific files.

```{r Period Subsets}

#extract period-specific sites and store them in separate data set 
Khabur_Ane_pred_Tuna_EBA1 <- Khabur_Ane_pred_Tuna[grep("EBA1", Khabur_Ane_pred_Tuna$Period), ]
Khabur_Ane_pred_Tuna_EBA2 <- Khabur_Ane_pred_Tuna[grep("EBA2", Khabur_Ane_pred_Tuna$Period), ]
Khabur_Ane_pred_Tuna_MBA <- Khabur_Ane_pred_Tuna[grep("MBA", Khabur_Ane_pred_Tuna$Period), ]
Khabur_Ane_pred_Tuna_LBA <- Khabur_Ane_pred_Tuna[grep("LBA", Khabur_Ane_pred_Tuna$Period), ]
Khabur_Ane_pred_Tuna_IA1 <- Khabur_Ane_pred_Tuna[grep("IA1", Khabur_Ane_pred_Tuna$Period), ]
Khabur_Ane_pred_Tuna_IA2 <- Khabur_Ane_pred_Tuna[grep("IA2", Khabur_Ane_pred_Tuna$Period), ]

#extract all sites with dating information
Khabur_Ane_pred_Tuna_dates <- Khabur_Ane_pred_Tuna %>%
  filter(!is.na(Khabur_Ane_pred_Tuna$Period)) 

#write data sets to shapefile
st_write(Khabur_Ane_pred_Tuna_EBA1, dsn = "Khabur_Ane_pred_Tuna_EBA1.shp", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_EBA2, dsn = "Khabur_Ane_pred_Tuna_EBA2.shp", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_MBA, dsn = "Khabur_Ane_pred_Tuna_MBA.shp", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_LBA, dsn = "Khabur_Ane_pred_Tuna_LBA.shp", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_IA1, dsn = "Khabur_Ane_pred_Tuna_IA1.shp", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_IA2, dsn = "Khabur_Ane_pred_Tuna_IA2.shp", append = FALSE)

st_write(Khabur_Ane_pred_Tuna_dates, dsn = "Khabur_Ane_pred_Tuna_dates.shp", append = FALSE)

st_write(Khabur_Ane_pred_Tuna_EBA1, dsn = "Khabur_Ane_pred_Tuna_EBA1.csv", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_EBA2, dsn = "Khabur_Ane_pred_Tuna_EBA2.csv", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_MBA, dsn = "Khabur_Ane_pred_Tuna_MBA.csv", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_LBA, dsn = "Khabur_Ane_pred_Tuna_LBA.csv", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_IA1, dsn = "Khabur_Ane_pred_Tuna_IA1.csv", append = FALSE)
st_write(Khabur_Ane_pred_Tuna_IA2, dsn = "Khabur_Ane_pred_Tuna_IA2.csv", append = FALSE)

```
